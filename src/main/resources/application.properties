# La URL del contenedor de Ollama
spring.ai.ollama.base-url=http://localhost:11434 
# Modelo de LLM con el que nos comunicamos y que usará nuestro contenedor Ollama de docker
spring.ai.ollama.chat.model=mistral:latest
# Modelo de LLM para la creacion de los registros de la base de datos de chromaDB que tenemos en el contenedor de docker
spring.ai.ollama.embedding.model=all-minilm

# La URL del contenedor de ChromaDB
spring.ai.vectorstore.chroma.base-url=http://localhost:8000
# Nombre de la coleccion donde tenemos guardados los registros que previamente hay que crear en el contenedor
spring.ai.vectorstore.chroma.collection-name=prueba

spring.ai.vectorstore.chroma.tenant=default
#spring.ai.vectorstore.chroma.database=default